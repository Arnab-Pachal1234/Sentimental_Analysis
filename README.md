# IMDB Sentiment Analysis using Naive Bayes ğŸ¬ğŸ“

This project performs **sentiment analysis** on IMDB movie reviews using **Natural Language Processing (NLP)** techniques and **Naive Bayes classifiers**. The goal is to classify movie reviews as **positive** or **negative** and compare the performance of different Naive Bayes variants.

---

## ğŸ“‚ Dataset

* **File name**: `IMDB.csv`
* **Description**: Contains 50,000 movie reviews with sentiment labels (positive/negative).
* **Sample used**: 10,000 randomly sampled reviews for faster computation.
* âš ï¸ Ensure `IMDB.csv` is present in the project root directory before running the code.

---

## ğŸ› ï¸ Technologies & Libraries Used

* Python 3.x
* NumPy â€“ Numerical computations
* Pandas â€“ Data manipulation
* NLTK â€“ Natural Language Processing
* Scikit-learn â€“ Machine learning algorithms

```bash
pip install numpy pandas nltk scikit-learn
```

---

## ğŸ” Workflow Overview

### 1ï¸âƒ£ Data Loading & Inspection

* Load CSV data using Pandas
* Inspect dataset shape and head
* Check for null values
* View summary statistics

### 2ï¸âƒ£ Data Cleaning

* Remove HTML tags using Regular Expressions
* Convert text to lowercase
* Remove special characters
* Remove stopwords using NLTK
* Apply stemming using Porter Stemmer

### 3ï¸âƒ£ Feature Extraction

* Convert text into numerical features using **Bag of Words** (`CountVectorizer`)
* Final feature matrix shape: `(10000, 36187)`
* Labels converted to 0 (negative) and 1 (positive)

### 4ï¸âƒ£ Train-Test Split

* Split data into **training (80%)** and **testing (20%)** sets
* Shapes:

  * `X_train`: (8000, 36187)
  * `X_test`: (2000, 36187)
* `y_train`: (8000,)
* `y_test`: (2000,)

### 5ï¸âƒ£ Model Training

* Implemented three variants of **Naive Bayes**:

  1. GaussianNB
  2. MultinomialNB
  3. BernoulliNB
* Fit each model on the training data

### 6ï¸âƒ£ Model Evaluation

* Predicted sentiments for test data
* Calculated accuracy for each model

---

## ğŸ“Š Model Performance (Accuracy)

| Model         | Accuracy |
| ------------- | -------- |
| GaussianNB    | 63.35%   |
| MultinomialNB | 83.70% âœ… |
| BernoulliNB   | 80.85%   |

---

## ğŸ“Œ Conclusion

* **Multinomial Naive Bayes** performed the best for text classification with word frequency features.
* GaussianNB is less effective because it assumes normally distributed features, which is not ideal for sparse text data.
* BernoulliNB works well for binary features but slightly lower than MultinomialNB.

---

## ğŸ› ï¸ How to Run

```bash
python main.py
```

> Or run the notebook cell-by-cell if using **Jupyter Notebook**.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ IMDB.csv
â”œâ”€â”€ main.py
â”œâ”€â”€ README.md
```

---

## â­ Key Takeaways

* Proper **text preprocessing** improves model performance.
* **Bag of Words** and **Naive Bayes** are effective for sentiment classification.
* Model choice matters depending on the **feature distribution**.

---

## ğŸ“œ License

This project is for **educational purposes only**.

---

## ğŸš€ Happy Learning & Sentiment Analysis!
